{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Define function to send a prompt to GPT chat and receive a response\n",
    "def gpt_chat(prompt, model, temperature=0.1, max_tokens=1000):\n",
    "    kwargs = {\n",
    "        \"stop\": None,\n",
    "        \"n\": 1,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"best_of\": 1,\n",
    "        \"logprobs\": None,\n",
    "        \"echo\": False,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "    \n",
    "\n",
    "# Example usage: initiate GPT chat using the \"davinci\" model\n",
    "\n",
    "\n",
    "\n",
    "def gpt_text_davinci_002():\n",
    "    prompt = str(input(\"Enter your prompt: \"))\n",
    "    model='text-davinci-002'\n",
    "    response = gpt_chat(prompt, model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "import openai\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Define function to send a prompt to GPT chat and receive a response\n",
    "def gpt_chat(prompt, model='gpt-3.5-turbo-0301', temperature=0.1, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=None,\n",
    "        n=1,\n",
    "        presence_penalty=0,\n",
    "        frequency_penalty=0,\n",
    "        best_of=1,\n",
    "        logprobs=None,\n",
    "        echo=False,\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Define function to initiate GPT chat using the \"davinci\" model and return the response\n",
    "def gpt_text_davinci_002():\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response = gpt_chat(prompt)\n",
    "    \n",
    "    return response, sep='\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=gpt_text_davinci_002( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# print(f\"{name} is {age} years old.{sep}He likes {hobby}\")\n",
      "\n",
      "# print(name + ' is ' + str(age) + ' years old.' + sep + 'He likes ' + hobby)\n",
      "\n",
      "# print('{} is {} years old.{}He likes {}'.format(name, age, sep, hobby))\n",
      "\n",
      "# print(f'{name} is {age} years old.{sep}He likes {hobby}')\n",
      "\n",
      "# print('{0} is {1} years old.{2}He likes {3}'.format(name, age, sep, hobby))\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=4000\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "def gpt_text_davinci_002():\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response = generate_text(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5w\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def gpt_text_davinci_003v1():\n",
    "    output=\"\"\n",
    "    #print(len(output))\n",
    "    if len(output) == 0:\n",
    "        prompt = input(\"Enter your prompt: \")\n",
    "        response = openai.Completion.create(\n",
    "            engine='text-davinci-003',\n",
    "            prompt=prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=3400)\n",
    "    \n",
    "    else:\n",
    "        prompt=input(\"Enter your prompt: \")\n",
    "        \n",
    "        promptwc = output+prompt\n",
    "        response = openai.Completion.create(\n",
    "            engine='text-davinci-003',\n",
    "            prompt=prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=3400\n",
    "    )\n",
    "    output=response.choices[0].text\n",
    "    #print(len(output))\n",
    "    return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5w\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def gpt_text_davinci_003v1():\n",
    "    output=\"\"\n",
    "    #print(len(output))\n",
    "\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=3400)\n",
    "\n",
    "    output=response.choices[0].text\n",
    "    #print(len(output))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first prompt\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# inicjalizacja zmiennej globalnej\n",
    "global_output = \"\"\n",
    "\n",
    "def gpt_text_davinci_003p1():\n",
    "    global global_output\n",
    "\n",
    "    prompt = input(\"Enter your first prompt: \")\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=3400)\n",
    "\n",
    "    output=response.choices[0].text\n",
    "\n",
    "    # zapisanie wartości do zmiennej globalnej\n",
    "    global_output = output\n",
    "\n",
    "    return f'first prompt\\n{output}\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next prompt\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# inicjalizacja zmiennej globalnej\n",
    "global_output = \"\"\n",
    "\n",
    "def gpt_text_davinci_003p2():\n",
    "    global global_output\n",
    "\n",
    "    prompt = global_output+input(\"Enter your next prompt: \")\n",
    "    print(prompt)\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=3000)\n",
    "\n",
    "    output=response.choices[0].text\n",
    "\n",
    "    # zapisanie wartości do zmiennej globalnej\n",
    "    global_output = output\n",
    "\n",
    "    return f'next prompt \\n{output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context cleaner\n",
    "global_output =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "#context checker\n",
    "print(len(global_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "r = 1000\n",
      "t = np.linspace(0, 2*np.pi, 100)\n",
      "x = r*np.cos(t)\n",
      "y = r*np.sin(t)\n",
      "plt.plot(x, y)\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(global_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context():\n",
    "    if len(global_output) == 0:\n",
    "          return gpt_text_davinci_003p1()\n",
    "    else:\n",
    "          return gpt_text_davinci_003p2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "r = 1000\n",
      "t = np.linspace(0, 2*np.pi, 100)\n",
      "x = r*np.cos(t)\n",
      "y = r*np.sin(t)\n",
      "plt.plot(x, y)\n",
      "plt.show()zmień promień na 100\n",
      "next prompt \n",
      "\n",
      "\n",
      "r = 100\n",
      "t = np.linspace(0, 2*np.pi, 100)\n",
      "x = r*np.cos(t)\n",
      "y = r*np.sin(t)\n",
      "plt.plot(x, y)\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHaha, that's a good one!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_text_davinci_003v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "3#\n",
    "import openai\n",
    "\n",
    "def gpt_chat(prompt, engine=\"gpt-3.5-turbo\", temperature=0.1, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=None,\n",
    "        n=1,\n",
    "        presence_penalty=0,\n",
    "        frequency_penalty=0,\n",
    "        best_of=1,\n",
    "        logprobs=None,\n",
    "        echo=False,\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def gpt_text_davinci_002():\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response = gpt_chat(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def gpt_chat(prompt, engine=\"davinci\", temperature=0.5, max_tokens=150):\n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=None,\n",
    "        n=1,\n",
    "        presence_penalty=0,\n",
    "        frequency_penalty=0,\n",
    "        best_of=1,\n",
    "        model=\"text-davinci-002\",\n",
    "        prompt_prefix=\"\",\n",
    "        prompt_suffix=\"\",\n",
    "        response_format=\"json\",\n",
    "        stream=False,\n",
    "        stop_sequence=None,\n",
    "        timeout=None,\n",
    "        user=None,\n",
    "        **{\n",
    "            \"completion\": {\n",
    "                \"chat\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
