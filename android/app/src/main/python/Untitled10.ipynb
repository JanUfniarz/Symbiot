{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2EYhkX0LPW4",
        "outputId": "f5d0f555-458c-449c-b726-3eecdddd81eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.27.2)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (3.8.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (5.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZQvDVQ6ULWto"
      },
      "outputs": [],
      "source": [
        "#first prompt\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# inicjalizacja zmiennej globalnej\n",
        "global_output = \"\"\n",
        "\n",
        "def gpt_text_davinci_003p1():\n",
        "    global global_output\n",
        "\n",
        "    prompt = input(\"Enter your first prompt: \")\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.1,\n",
        "        max_tokens=3400)\n",
        "\n",
        "    output=response.choices[0].text\n",
        "\n",
        "    # zapisanie wartości do zmiennej globalnej\n",
        "    global_output = output\n",
        "\n",
        "    #output_history[len(output_history)+1] = output\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0sAy5Zj6LqXl"
      },
      "outputs": [],
      "source": [
        "#next prompt\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# inicjalizacja zmiennej globalnej\n",
        "global_output = \"\"\n",
        "\n",
        "def gpt_text_davinci_003p2():\n",
        "    global global_output\n",
        "\n",
        "    prompt = global_output+' '+input(\"Enter your next prompt: \")\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.1,\n",
        "        max_tokens=3000)\n",
        "\n",
        "    output=response.choices[0].text\n",
        "\n",
        "    # zapisanie wartości do zmiennej globalnej\n",
        "    global_output = output\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#profesor\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# inicjalizacja zmiennej globalnej\n",
        "global_output = \"\"\n",
        "\n",
        "def gpt_text_davinci_003prof():\n",
        "    global global_output\n",
        "\n",
        "    prompt = 'Imagine that you are a professor and a student comes to you with the following questions'+global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.1,\n",
        "        max_tokens=3000)\n",
        "\n",
        "    output=response.choices[0].text\n",
        "\n",
        "    # zapisanie wartości do zmiennej globalnej\n",
        "    global_output = output\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#student\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# inicjalizacja zmiennej globalnej\n",
        "global_output = \"\"\n",
        "\n",
        "def gpt_text_davinci_003stud():\n",
        "    global global_output\n",
        "\n",
        "    prompt = 'Imagine you are a student and you have to do a project on'+input(\"Enter your 1 prompt: \")+' write the questions you will ask the professor'\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.1,\n",
        "        max_tokens=3000)\n",
        "\n",
        "    output=response.choices[0].text\n",
        "\n",
        "    # zapisanie wartości do zmiennej globalnej\n",
        "    global_output = output\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sukces????\n",
        "def gpt_text_davinci_003_profvsstud():\n",
        "      #global output_history\n",
        "      if len(global_output) == 0:\n",
        "            p1=gpt_text_davinci_003stud()\n",
        "            history[len(history)] = p1\n",
        "            return p1\n",
        "      else:      \n",
        "            p2=gpt_text_davinci_003prof()\n",
        "            history[len(history)] = p2\n",
        "            return p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fantomGPT():\n",
        "    import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# inicjalizacja zmiennej globalnej\n",
        "global_output = \"\"\n",
        "\n",
        "def gpt_text_davinci_003stud():\n",
        "    global global_output\n",
        "\n",
        "    prompt = chatmode() +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.1,\n",
        "        max_tokens=3000)\n",
        "\n",
        "    output=response.choices[0].text\n",
        "\n",
        "    # zapisanie wartości do zmiennej globalnej\n",
        "    global_output = output\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatmode():\n",
        "    chatmodelist=['tagger','ask','explain','expand','coder','filter','extractor','search','abstract','control','environment','XMLtager','shorter','modeselect']   \n",
        "    tagger=[prompt = 'classifide and make tags to this' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=500]\n",
        "    ask=[prompt = 'ask questions about it' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    explain=[prompt = 'Answer these questions as a specialist:' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    expand=[prompt = 'elaborate' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=3000]\n",
        "    \n",
        "    coder=[prompt = 'write code in python that' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='code-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=7000]\n",
        "    \n",
        "    filter=[prompt = 'write your answer in the form of a list that can be easily loaded' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    extractor=[prompt = 'how to get information about' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    search=[prompt = 'Compare two lists of tags and give those that match each other in context' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    abstract=[prompt = 'write an abstract to ' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    \n",
        "    XMLtagger=[prompt = 'Add tags in XML to this data ' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    shorter=[prompt = 'Add tags in XML to this data ' +global_output\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]\n",
        "    modeselect=[prompt = 'Which of these modes best fits the task at hand ?' +global_output#\n",
        "    print(prompt)\n",
        "    response = openai.Completion.create\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_tokens(prompt):\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"davinci\", \n",
        "      prompt=prompt, \n",
        "      max_tokens=1\n",
        "    )\n",
        "    return len(response.choices[0].text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def prompt_token_count(prompt):\n",
        "    api_key = \"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\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        'prompt': prompt,\n",
        "        'max_tokens': 0,\n",
        "        'temperature': 0,\n",
        "        'n': 1,\n",
        "        'stop': '',\n",
        "    }\n",
        "\n",
        "    response = requests.post('https://api.openai.com/v1/davinci-codex/completions', headers=headers, data=json.dumps(data))\n",
        "    if response.status_code == 200:\n",
        "        tokens = response.json()['choices'][0]['text'].split()\n",
        "        return len(tokens)\n",
        "    else:\n",
        "        raise Exception(f'Request failed with status code {response.status_code}. Error message: {response.json()[\"error\"]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Request failed with status code 404. Error message: {'message': 'Invalid URL (POST /v1/davinci-codex/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Admin\\iron-man-ai\\iron-man-ai\\android\\app\\src\\main\\python\\Untitled9.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt_token_count(\u001b[39m\"\u001b[39;49m\u001b[39mnapisz funkcje w python która dodaje dwie wartości\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\Admin\\iron-man-ai\\iron-man-ai\\android\\app\\src\\main\\python\\Untitled9.ipynb Cell 12\u001b[0m in \u001b[0;36mprompt_token_count\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(tokens)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRequest failed with status code \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m. Error message: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mException\u001b[0m: Request failed with status code 404. Error message: {'message': 'Invalid URL (POST /v1/davinci-codex/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}"
          ]
        }
      ],
      "source": [
        "prompt_token_count(\"napisz funkcje w python która dodaje dwie wartości\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "You requested a length 0 completion, but you did not set the 'echo' parameter. That means that we will return no data to you. (HINT: set 'echo' to true in order for the API to echo back the prompt to you.)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Admin\\iron-man-ai\\iron-man-ai\\android\\app\\src\\main\\python\\Untitled9.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m count_tokens(\u001b[39m\"\u001b[39;49m\u001b[39mnapisz funkcje w python która dodaje dwie wartości\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\Admin\\iron-man-ai\\iron-man-ai\\android\\app\\src\\main\\python\\Untitled9.ipynb Cell 12\u001b[0m in \u001b[0;36mcount_tokens\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_tokens\u001b[39m(prompt):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     prompt_tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdavinci\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         logprobs\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlogprobs\u001b[39m.\u001b[39mtokens:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         ),\n\u001b[0;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[1;31mInvalidRequestError\u001b[0m: You requested a length 0 completion, but you did not set the 'echo' parameter. That means that we will return no data to you. (HINT: set 'echo' to true in order for the API to echo back the prompt to you.)"
          ]
        }
      ],
      "source": [
        "count_tokens(\"napisz funkcje w python która dodaje dwie wartości\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine that you are a professor and a student comes to you with the following questions\n",
            "\n",
            "1. What is the definition of clustering illusion?\n",
            "2. How does clustering illusion affect decision-making?\n",
            "3. What are some examples of clustering illusion?\n",
            "4. What is the curse of knowledge?\n",
            "5. How does the curse of knowledge affect decision-making?\n",
            "6. What are some examples of the curse of knowledge?\n",
            "7. How can we avoid the clustering illusion and the curse of knowledge when making decisions?\n",
            "8. What strategies can be used to mitigate the effects of the clustering illusion and the curse of knowledge?\n",
            "\n",
            "\n",
            "Answer: \n",
            "1. Clustering illusion is a cognitive bias in which people perceive patterns in random data. This can lead to false conclusions about the underlying cause of the data.\n",
            "\n",
            "2. Clustering illusion can lead to overconfidence in decision-making, as people may believe that they have identified a pattern in the data when in fact there is none. This can lead to poor decision-making as people may make decisions based on false assumptions.\n",
            "\n",
            "3. Examples of clustering illusion include seeing patterns in stock market prices, believing that a certain number is “lucky”, or believing that a certain outcome is more likely to occur than it actually is.\n",
            "\n",
            "4. The curse of knowledge is a cognitive bias in which people overestimate the knowledge of others. This can lead to poor decision-making as people may make decisions based on assumptions about what others know.\n",
            "\n",
            "5. The curse of knowledge can lead to poor decision-making as people may make decisions based on assumptions about what others know, which may not be accurate. This can lead to decisions that are not in the best interest of the organization.\n",
            "\n",
            "6. Examples of the curse of knowledge include assuming that everyone knows the same information, assuming that everyone has the same level of expertise, or assuming that everyone has the same level of understanding.\n",
            "\n",
            "7. To avoid the clustering illusion and the curse of knowledge when making decisions, it is important to be aware of these cognitive biases and to take steps to mitigate their effects. This can include gathering more data, seeking out different perspectives, and questioning assumptions.\n",
            "\n",
            "8. Strategies that can be used to mitigate the effects of the clustering illusion and the curse of knowledge include gathering more data, seeking out different perspectives, and questioning assumptions. Additionally, it is important to be aware of the potential for cognitive biases and to take steps to mitigate their effects.\n"
          ]
        }
      ],
      "source": [
        "print(gpt_text_davinci_003_profvsstud())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vlxee1zbL7XD"
      },
      "outputs": [],
      "source": [
        "def gpt_text_davinci_003_with_last_prompt_context():\n",
        "    \n",
        "    if len(global_output) == 0:\n",
        "          return gpt_text_davinci_003p1()\n",
        "    else:\n",
        "          return gpt_text_davinci_003p2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sukces\n",
        "def gpt_text_davinci_003_context_history():\n",
        "      #global output_history\n",
        "      if len(global_output) == 0:\n",
        "            p1=gpt_text_davinci_003p1()\n",
        "            history[len(history)] = p1\n",
        "            return p1\n",
        "      else:\n",
        "            p2=gpt_text_davinci_003p2()\n",
        "            history[len(history)] = p2\n",
        "            return p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'gpt_text_davinci_003_context_history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Admin\\iron-man-ai\\iron-man-ai\\android\\app\\src\\main\\python\\Untitled9.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/iron-man-ai/iron-man-ai/android/app/src/main/python/Untitled9.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gpt_text_davinci_003_context_history()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'gpt_text_davinci_003_context_history' is not defined"
          ]
        }
      ],
      "source": [
        "gpt_text_davinci_003_context_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "history={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'A', 2: 'B', 3: 'C'}\n"
          ]
        }
      ],
      "source": [
        "lista=[1,2,3]\n",
        "lista2=[\"A\",\"B\",\"C\"]\n",
        "x=dict(zip(lista,lista2))\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#hmmmm kurwa czemu ?\n",
        "def gpt_text_davinci_003_context_history():\n",
        "    x= gpt_text_davinci_003_with_last_prompt_context()\n",
        "    history[len(history)] = x\n",
        "    #return gpt_text_davinci_003_with_last_prompt_context()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "def mean_dict(x, y, z):\n",
            "  mean_dict = {}\n",
            "  mean_dict[1] = x\n",
            "  mean_dict[2] = y\n",
            "  mean_dict[3] = z\n",
            "  return mean_dict\n",
            "\n",
            "print(mean_dict('selfish', 'stingy', 'mean')) zmien nazyw zmiennych\n",
            "next prompt \n"
          ]
        }
      ],
      "source": [
        "print(gpt_text_davinci_003_context_history())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "napisz kod w python który zapisze kolejne wczytane zmienne w postaci słownika z numerem ich wywołania w funkcji według kolejności wywołania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: \"\\n\\n1. Co to jest torowanie lub efekt torowania?\\n2. Jakie są główne cechy tego zjawiska?\\n3. Jakie są przyczyny błędu poniesionych kosztów?\\n4. Jakie są skutki efektu utopionych kosztów?\\n5. Jakie są przyczyny irracjonalnej eskalacji?\\n6. Jakie są skutki błędu Concorde'a?\\n7. Jakie są sposoby zapobiegania tym zjawiskom?\\n8. Jakie są najlepsze praktyki w zakresie zarządzania tymi zjawiskami?\", 1: \"\\n\\nOdpowiedź:\\n1. Torowanie lub efekt torowania to zjawisko, w którym decydenci kontynuują inwestycje w projekty, które są już zaawansowane, nawet jeśli nie są one opłacalne.\\n2. Główne cechy tego zjawiska to: (1) decydenci kontynuują inwestycje w projekty, które są już zaawansowane, nawet jeśli nie są one opłacalne; (2) decydenci nie są w stanie zmienić swoich decyzji, ponieważ wkładają zbyt dużo czasu i środków w projekt; (3) decydenci nie są w stanie zmienić swoich decyzji, ponieważ wkładają zbyt dużo czasu i środków w projekt; (4) decydenci nie są w stanie zmienić swoich decyzji, ponieważ wkładają zbyt dużo czasu i środków w projekt; (5) decydenci nie są w stanie zmienić swoich decyzji, ponieważ wkładają zbyt dużo czasu i środków w projekt.\\n3. Przyczyny błędu poniesionych kosztów to: (1) brak wystarczającej wiedzy na temat projektu; (2) brak wystarczającego czasu na zbadanie projektu; (3) brak wystarczającego zrozumienia projektu; (4) brak wystarczającego zaangażowania w projekt; (5) brak wystarczającego zarządzania projektem.\\n4. Skutki efektu utopionych kosztów to: (1) zwiększone koszty projektu; (2) opóźnienia w realizacji projektu; (3) zmniejszenie jakości projektu; (4) zwiększenie ryzyka projektu; (5) zmniejszenie zwrotu z inwestycji.\\n5. Przyczyny irracjonalnej eskalacji to: (1) presja na szybkie wykonanie projektu; (2) presja na zwiększenie zakresu projektu; (3) presja na zwiększenie budżetu projektu; (4) presja na zwiększenie zaangażowania w projekt; (5) presja na zwiększenie zarządzania projektem.\\n6. Skutki błędu Concorde'a to: (1) zwiększone koszty projektu; (2) opóźnienia w realizacji projektu; (3) zmniejszenie jakości projektu; (4) zwiększenie ryzyka projektu; (5) zmniejszenie zwrotu z inwestycji.\\n7. Sposoby zapobiegania tym zjawiskom to: (1) wczesne wykrywanie problemów; (2) wczesne wykrywanie zmian; (3) wczesne wykrywanie nieprawidłowości; (4) wczesne wykrywanie nieefektywności; (5) wczesne wykrywanie nieopłacalności.\\n8. Najlepsze praktyki w zakresie zarządzania tymi zjawiskami to: (1) wczesne wykrywanie problemów; (2) wczesne wykrywanie zmian; (3) wczesne wykrywanie nieprawidłowości; (4) wczesne wykrywanie nieefektywności; (5) wczesne wykrywanie nieopłacalności; (6) wczesne wykrywanie nieprawidłowości w zarządzaniu projektem; (7) wczesne wykrywanie nieefektywności w zarządzaniu projektem; (8) wczesne wykrywanie nieopłacalności w zarządzaniu projektem; (9) wczesne wykrywanie nieprawidłowości w zarządzaniu zasobami; (10) wczesne wykrywanie nieefektywności w zarządzaniu zasobami; (11) wczesne wykrywanie nieopłacalności w zarządzaniu zasobami.\"}\n"
          ]
        }
      ],
      "source": [
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiRAFSkGLeWI",
        "outputId": "d54492b3-094b-4238-8902-ccd27406409b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#context checker\n",
        "print(len(global_output))\n",
        "print(global_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EW87701iL-L4"
      },
      "outputs": [],
      "source": [
        "#context cleaner\n",
        "global_output =\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(output_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnatRCOULmDg",
        "outputId": "75c63fde-e6ab-4ebd-daf3-6a0dbaab0718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first prompt\n",
            "\n",
            "\n",
            "slownik = {}\n",
            "\n",
            "def funkcja(zmienna):\n",
            "    slownik[len(slownik)] = zmienna\n",
            "\n",
            "funkcja('a')\n",
            "funkcja('b')\n",
            "funkcja('c')\n",
            "\n",
            "print(slownik) # {0: 'a', 1: 'b', 2: 'c'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#wywołanie\n",
        "print(gpt_text_davinci_003_with_last_prompt_context())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#context checker\n",
        "print(len(global_output))\n",
        "print(global_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_input():\n",
        "    global input_history\n",
        "\n",
        "    # wyświetlenie informacji o numerze wywołania i wczytanie zmiennej\n",
        "    call_number = len(input_history) + 1\n",
        "    prompt = input(f\"Enter input for call {call_number}: \")\n",
        "\n",
        "    # zapisanie wczytanej wartości do słownika\n",
        "    input_history[call_number] = prompt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
